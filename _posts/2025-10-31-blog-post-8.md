---
title: 'The Harm of the Matter.'
date: 2025-10-31
permalink: /posts/2025/10/blog-post-8/
tags:
  - case study
  - blog
  - ethics
  - generative ai
  - datasets
  - bias
  - mitigating harm
  - machine learning

---

How does AI bias cause harm? Who is impacted by it?

**Case Study:**  
["Understanding Potential Sources of Harm throughout the Machine Learning Life Cycle"](https://mit-serc.pubpub.org/pub/potential-sources-of-harm-throughout-the-machine-learning-life-cycle/release/2)

---

This article identifies seven potential sources of harm, and proceeds to explain each of them distinctly. The goal of this is to recognize not that these issues are mutually exclusive, rather, that they are separate issues ultimately originating from the same source. The bias in machine learning is a broad topic, but nonetheless important to acknowledge. 

The bottom of this article provides 6 questions to encourage further discussion. I will be picking the 3 that I find most interesting to answer through this blog post.

Q1: Can you come up with an additional example of each source of harm?
A1: For this, I'll list each source of harm that was listed in the article and put what I think of beside it.

Historical Bias: I think Word Embeddings is what I think of most when I think of Historical Bias. I struggled to think of another example for this.
Representation Bias: Historically, colonizers (particularly Great Britian and America) loved depicting Africa to be this completely undeveloped, uncivilized place. That propaganda feels almost 1:1 to the way AI likes to generate what it thinks Africa looks like. That does tie in to the example that they gave, but I really think AI has a propaganda issue. 
Measurement Bias: I think about how healthcare AI likes to operate. Then again, it's sort of built the way it is so that healthcare companies profit more off of it's easy denials, but that's a topic for another day. The idea of giving a machine a request to fund a life-saving surgery based on minimal information is insane to me. 
Aggregation Bias: I struggled with this one for some reason. I might be sleepy, though. 
Learning Bias: I feel like this is sort of inevitable, regardless of what you're making an AI for. Nobody has infinite resources and computing power, there will always be something that you have to sacrifice for the sake of another. So I guess I would link this to material constraints. 
Evaluation Bias: I think of human error. Humans can't make things perfect on the first go, and we know that, and so we set benchmarks for ourselves for success rates. But being conscious of these reports makes us act different inherently, so theres a bias already.
Deployment Bias: I think a lot of commonly used AI falls under this. Dall-E started as this goofy sort of thing to generate images, but as AI image generation started to develop, it began to be used by some people as a way to "replace artists." That changed the way that a lot of it has developed I think. 

Q2: Can you come up with additional ways to detect and/or mitigate each source of harm?
A2: I guess it depends. I've said before how I think that having more diversity in the CS field would be really helpful, that was the voices currently being underrepresented could be able to point out when something is wrong sooner. That way we don't have to wait until deployment for things to get noticed, when the damage has already been done. I think that more thorough testing standards can always help, and we should really hold AI to higher standards before we try to deploy it. We shouldn't let a 67% accuracy rating potentially ruin the lives of thousands of people. 

Q3: Can you think of other sources of harm that arenâ€™t captured in this case study?
A3: I feel bad leaving this post off on kind of a lame note - I had a paragraph written about confirmation bias, but then I realized that it was touched on in Deployment Bias. But the types of bias discussed in the article were pretty good umbrellas where I think a lot of sub-problems can fall into. (Man, AI has a lot of problems. It's depressing.) If I was more in-tune with AI algorithm research, I'd probably have more insight on the matter. It talks about in 3.8 how recognizing these biases requires an understanding of how it's developed in the domain over time. Come back to me in a couple years and I might have a better answer. 

---

I think this article provided good insight and a good method of identifying the harm that AI can contribute to. I think discussions like this are super important to have, especially for people who aren't *studying* CS, but still have an interest in utilizing AI. There are pros and cons to everything, and when you weigh in structual harm like this, the cons start to feel a lot heavier. Then again, I'm very pessimistic about AI. That's my own bias.

I would like to pose a new discussion question to leave this blog post off with: "Which area of harm do you think can be mitigated best? Can any of them really be mitigated? Why or why not?"