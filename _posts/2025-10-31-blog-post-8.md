---
title: 'The Harm of the Matter.'
date: 2025-10-31
permalink: /posts/2025/10/blog-post-8/
tags:
  - case study
  - blog
  - ethics
  - generative ai
  - datasets
  - bias
  - mitigating harm
  - machine learning

---

How does AI bias cause harm? Who is impacted by it?

**Case Study:**  
["Understanding Potential Sources of Harm throughout the Machine Learning Life Cycle"](https://mit-serc.pubpub.org/pub/potential-sources-of-harm-throughout-the-machine-learning-life-cycle/release/2)

---

This article identifies seven potential sources of harm, and proceeds to explain each of them distinctly. The goal of this is to recognize not that these issues are mutually exclusive, rather, that they are separate issues ultimately originating from the same source. The bias in machine learning is a broad topic, but nonetheless important to acknowledge. 

The bottom of this article provides 6 questions to encourage further discussion. I will be picking the 3 that I find most interesting to answer through this blog post.

I will be listing each source of harm that this article discussed, and putting my thoughts beside it. 

Historical Bias: This type of bias comes from real-world factors that can still inflict harm on the populatons they're about. I think Word Embeddings is what I think of most when I think of Historical Bias: where if you look up "CEO," "Manager," etc., you will usually be met with generated images of white men. This comes from a systematic problem, but it's not a good thing to perpetuate.

Representation Bias: Historically, colonizers (particularly Great Britian and America) loved depicting Africa to be this completely undeveloped, uncivilized place. That propaganda feels almost 1:1 to the way AI likes to generate what it thinks Africa looks like. That does tie in to the example that they gave, but I really think AI has a propaganda issue. 

Measurement Bias: This type of bias uses factors and labels to try and quantify something. I think about how healthcare AI likes to operate. Then again, it's sort of built the way it is so that healthcare companies profit more off of it's easy denials, but that's a topic for another day. The idea of giving a machine a request to fund a life-saving surgery based on minimal information is insane to me. 

Aggregation Bias: I struggled with this one for a while. Aggregation tries to fit data into one category where there's too much diversity to do so. I kind of think of Roblox for this strangely enough. Roblox is a platform primarily comprised of children, but Roblox really wanted to push a dating app onto their platform for some reason. They want to make Roblox like an everything-in-one platform, but it's awkward and (predictably) doesn't work. 

Learning Bias: This happens when the model weighs certain data over others to account for gaps. I feel like this is sort of inevitable, regardless of what you're making an AI for. Nobody has infinite resources and computing power, there will always be something that you have to sacrifice for the sake of another. So I guess I would link this to material constraints. 

Evaluation Bias: This is when benchmark data doesn't represent the population is used for. For this, I think of human error. Humans can't make things perfect on the first go, and we know that, and so we set benchmarks for ourselves for success rates. But being conscious of these reports makes us act different inherently, so theres a bias already.

Deployment Bias: I think a lot of commonly used AI falls under this. Dall-E started as this goofy sort of thing to generate images, but as AI image generation started to develop, it began to be used by some people as a way to "replace artists." That changed the way that a lot of it has developed I think. 

When thinking about alternate solutions to mitigating or stopping bias, I don't know if I have much more insight to this. I've said before how I think that having more diversity in the CS field would be really helpful, that was the voices currently being underrepresented could be able to point out when something is wrong sooner. That way we don't have to wait until deployment for things to get noticed, when the damage has already been done. I think that more thorough testing standards can always help, and we should really hold AI to higher standards before we try to deploy it. We shouldn't let a 67% accuracy rating potentially ruin the lives of thousands of people. 

Lastly, thinking on alternate types of bias, I feel a little bad leaving this post off on kind of a lame note - I had a paragraph written about confirmation bias, but then I realized that it was touched on in Deployment Bias. But the types of bias discussed in the article were pretty good umbrellas where I think a lot of sub-problems can fall into. (Man, AI has a lot of problems. It's depressing.) If I was more in-tune with AI algorithm research, I'd probably have more insight on the matter. It talks about in 3.8 how recognizing these biases requires an understanding of how it's developed in the domain over time. Come back to me in a couple years and I might have a better answer. 

---

I think this article provided good insight and a good method of identifying the harm that AI can contribute to. I think discussions like this are super important to have, especially for people who aren't *studying* CS, but still have an interest in utilizing AI. There are pros and cons to everything, and when you weigh in structual harm like this, the cons start to feel a lot heavier. Then again, I'm very pessimistic about AI. That's my own bias.

I would like to pose a new discussion question to leave this blog post off with: "Which area of harm do you think can be mitigated best? Can any of them really be mitigated? Why or why not?"

I think there are ways to mitigate these things, but I don't know how to balance practicality with theoretical solutions. It seems difficult. 