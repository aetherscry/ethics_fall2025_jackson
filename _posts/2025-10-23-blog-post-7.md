---
title: 'The Algorithm is only as good as the Teacher.'
date: 2025-10-23
permalink: /posts/2025/10/blog-post-7/
tags:
  - case study
  - blog
  - ethics
  - generative ai
  - ai
  - social justice
  - inclusivity

---

Racial and cultural bias in AI algorithms. 

**Case Study:**  
["AI’s Regimes of Representation: A Community-Centered Study of Text-to-Image Models in South Asia"](https://mit-serc.pubpub.org/pub/bfw5tscj/release/3?readingCollection=65a1a268)

---

This is a really fascinating case study on the cultural bias deeply imbedded into AI. The people training up many of these powerful generation models are westerners with a very narrow view of other parts of the world. This reading exposes that bias, and talks about the lack of representation for many cultures.  

The end of the article presents some questions for discussion, and I really like this particular set. I'll be going through and giving my thoughts on each point.

Q1: What does cultural representation mean to you, and how might this definition and your experience with past representation impact how you would evaluate representation of your identity in generative AI models? What aspects of your identity do you think you would center when evaluating representations in AI model output?

A1: I am someone who was not raised with much culture. I was raised with Christian holidays, but my parents aren't particularly religious. My great aunt is someone who loves geneology, and I can verify my ancestors are primarily from Sweeden and Norway. I'm very, *very* white. But I have found a home with queer culture and my trans identity, it's something very important to me. I almost worry though what would happen if I asked AI to generate a pride parade or something. I feel like it would be completely dominated by white gay men, honestly. And I worry that asking AI to generate a transgender person would get really stereotypical. 

Q2: What do you think is the role of small-scale qualitative evaluations for building more ethical generative AI models? How do they compare to larger, quantitative, benchmark-style evaluations?

A2: Small-scale qualititative evaluations catch bad data, I guess. It shows where theres an underrepresentation in the data you have provided it, and continuing to feed it this skewed data will only make it worse over time. It's like putting paint over a hole in the wall -- someone is going to find it eventually anyway. It's a weak point. 

Q3: Participants in this study shared “aspirations” for generative AI to be more inclusive, but also noted the tensions around making it more inclusive. Do you think AI can be made more globally inclusive?

A3: I've definitely said in previous blog posts that I'm a very pessimistic man. Unless these AI developers start losing money about it, they will not care enough to fix it. AI has been tainted enough by corporations, I genuinely expect nothing but the worst from AI developers for the most part. It feels like its become something very far from what it was meant to be. 

Q4: What mitigations do you think developers could explore to respond to some of the concerns raised by participants in this study?

A4: I'm a little confused by the phrasing of this question, honestly. What I interpret it asking is essentially "What could developers do to fix the problem of underrepresentation," but I feel like theres more than one way to interpret this question.

Anyway, I also think this depends. I believe that images and data should only be taken from purely public, free-to-use sources or art/artists that have given express consent for their work to be used for image generation. If it's possible to do a total overhaul *with* ethical sourcing, I think they should take the time to get more diverse sources instead of scraping from primarily white-centered data sets. I know it's probably not *realistic* exactly, but thats what I think would be *just*.

Q5: As mentioned in this case study, representation is not static; it changes over time, is culturally situated, and varies greatly from person to person. How can we “encode” this contextual and changing nature of representation into models, datasets, and algorithms? Is the idea of encoding at odds with the dynamism and fluidity of representation?

A5: It would be helpful to have more people from these different cultures in these fields. Even if we couldn't possible get a representative for every single culture, we could at least start the conversation about how where to look for information outside of our direct perspective. I'm sure theres great resources on where to look, but it might depend on the culture. 

Q6: How can we learn from the history of technology and media to build more responsible and representative AI models?

A6: We should be able to learn not only why things work, but also why some things don't work. It would prevent repeat errors for starters. But it can also reveal patterns in where weak points tend to pop up. Like representation, for starters. If we have a consistent problem of racial and gender bias, we should be able to adapt a more thorough method of checking for that problem in new tech before releasing an algorithm to the publiic. We can also look at the negative repercussions of tech and try to create safeguards to minimize damage as much as possible. 

---

I always knew about the underrepresentation of minorities in the Computer Science scene. I grew up female presenting, and in my AP Computer Science class, I was the only female-presenting person in there besides my teacher. But I never thought about scenarios like this, where an entire culture is neglected in the giant piles of knowledge shoved in to LLMs and image generation. We (unfortunately) hear all the time about AI being biased against marginalized people, but this is something else entirely. This is neglecting things and stereotyping others outside of the direct experiences or knowledge of those in charge.  

I would like to pose a new question for conversation after this reading. It's something I don't think I have a direct answer to, but something I've been thinking about: I believe that AI can only be as good as the data put in to it. But humans aren't perfect, that's part of life. Do you think it's possible for an AI to reach a point of complete non-bias? More importantly, do you see it happening in your lifetime?